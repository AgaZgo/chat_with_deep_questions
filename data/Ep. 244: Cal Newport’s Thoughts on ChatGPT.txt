Episode title: Cal Newportâ€™s Thoughts on ChatGPT

Chapter 1: Cal_s intro
So that is the deep question I want to address today. How does chat GPT work and how worried should we be about it? I'm Cal Newport and this is Deep Questions, the show about living and working deeply in an increasingly distracted world. I'm here on my Deep Work HQ, joined once again by my producer, Jesse. So Jesse, you may have noticed that we have been receiving a lot of emails in the last few months about chat GPT and related AI technologies. And our listeners want to know my thoughts on this, right? I'm a computer scientist. I've thought about the intersection of technology and society and I've been silent about it. I can reveal the reason why I've been silent about it is that I've been working on a big article for the New Yorker about exactly this technology, how it works and its implications for the world. And my general rule is when I'm writing an article, I don't talk about that subject publicly until the article is done. I mean, that's basic journalistic practice, but actually Jesse, I've never told this story, but that rule was really ingrained in me when I was in college. So when I was coming up as a young writer, you know, I got started pretty early, wrote my first book in college. Yeah. I was commissioned to write something for the New York Times. I don't remember exactly what it was, maybe an op-ed, something to do with college students or something like this. And I had an early blog at that time. And I wrote something on the blog like, hey, this exciting, I'm going to write an article from the New York Times. Like I put the short email on there, it was like, yeah, we'd love to have you write the piece or something. And that editor went ballistic. Really? Oh yeah. Cancel the piece. Cancel the piece. Shoot me out. Now this was early internet, right? I mean, this was 2004, probably. So I don't know. Maybe it felt more like a breach than ever since then. If I'm writing an article, did you ever talk to that editor again? No. I ended up writing a lot for the Times, but not really until 2012. Was that your going to be your first big splash? That would have been my first big splash. Were you like depressed for a couple of days? A little shook up. And then starting with so good they can ignore you and going forward, I had a really good relationship with the Times, especially through digital minimalism over in tons of articles for him. But there was a lesson learned. So I've written now the day that we're recording this podcast, April 13th, my new article of New Yorker has been published. So I am free. The gag order has been lifted. And we can get into it when it comes to chat GPT. In fact, I'll even load the article up here on the screen. For those who are watching, you will see this on the screen. If you're not watching, you can watch at youtube.com slash CalNuPort Media. Look for episode 244. You can also find that at the deeplife.com episode 244. Here Jesse is the long awaited article. The title is What Kind Of Mind Does Chat GPT Have? The subhead is large language models seem startingly intelligent, but what's really happening under the hood? And it's a big long article. So it's good. I'm excited to read it. Yeah, so we can talk chat GPT. I mean, you probably haven't been following it too closely. Just based on our conversation. Some people really in the weeds and some people don't want to know. I'm guessing you're not in the weeds on chat GPT, but it could be wrong. No, I'm not in the weeds at all. I listened to a few of the hard fork episodes on it. That was about it. And what was the tone of those episodes? They were given some examples of what it was when it first came out. Probably listened to them like six weeks ago. Yeah. Yeah, that was kind of it. Well, so I'll give a quick primer then before we get into the guts of what I want to talk about today. So chat GPT is a chat bot. You can sign up for an account at OpenAI and it's a web interface. You type in questions and chat GPT or prompts or request and chat GPT response types text back like you're talking to someone over Slack or instant messenger. So this was released in November, late November of last year. And almost immediately people began circulating online screenshots of particularly impressive interactions, particularly funny interactions that they had with chat GPT. Here's one of the first ones to go viral. I talk about this one in my article. So here's a tweet of a screenshot that went along. This was from a software developer named Thomas Pacek. And he asked chat GPT the following, write a biblical verse in the style of the King James Bible explaining how to remove a peanut butter sandwich from a VCR. That GPT rose to the challenge and wrote a response that begins and it came to pass that a man was troubled by a peanut butter sandwich for had been placed within his VCR. And he knew not how to remove it. And he cried out to the Lord saying, Oh Lord, how can I remove this sandwich from a VCR for a destuck fast and will not budge. And the response goes on. Here's another early viral example of chat GPT's prowess. This was a, someone named Riley Goodside who asked chat GPT to write a sign failed scene with Jerry needs to learn the bubble sort algorithm. And chat GPT once again rose to the occasion. A not a properly formatted script but has some of the aspects of it. It opens in amongst cafe. It says Jerry is sitting at the counter with George. Jerry says I can't believe I have to learn to bubble sort algorithm from a computer science class. George laughs. Bubble sort. That's the most basic sort algorithm there is. Even a monkey could do it. Audience laughs. Jerry. Yeah. Well, I'm not a monkey. I'm a comedian. And then the screen, the scene goes on from there. All right. So this is the type of thing chat GP can do. These impressively perceptive answers to pretty esoteric requests. Now if you go back and actually watch the media cycle around chat GPT, which I have to say is driven very strongly by Twitter. I think the fact that anyone can sign up for an account and that screenshots of your interactions can be easily embedded in the Twitter really helped get the hype cycle around this technology, spinning much more furiously than it has for past artificial intelligence innovations. Anyways, if you go back and look at this media cycle, it took a week or two before the tone shifted from exuberance and humor. Like if you look at this example, it just gave about sign fell. The tweet says, actually not that one. I meant of the VCR. The tweet says, I'm sorry, I simply cannot be cynical about technology that can accomplish this. So it went from this sort of exuberance and happiness to something that was a little bit more distressing. Here's a couple of examples I want to bring up here. Here is an article from NBC News. The headline is chat GPT passes MBA exam given by a Wharton professor. Uh oh, that got people worried. Here is another article from around this period from Time magazine. Headlined, he used AI to publish a children's book in a weekend. Artist are not happy about it. It details a product design manager who used chat GPT to write all the text of a book, which he then self published on Amazon and started selling. A bit of a stunt, but it implied certain types of future scenarios, which this technology was taking away creative work that really made people unsettled. As we get closer to the current period, I would say the tone shifted since the New Year, in particular coming in the March and April, the tone shifted towards one of alarm, not just about the focused economic impacts that are possible with this type of technology, but some of the bigger societal, if not civilization level impacts of these type of technologies. I would say one article that really helped set this tone was this now somewhat infamous Kevin Russe piece from the New York Times that is titled a conversation with Bing's chat box. Let me deeply unsettled. Being released at Chatbot after Chat GPT based on a very similar underlying technology, Kevin Russe was, I guess, beta testing or using this new tool and fell into this really sort of dark conversation with the chatbot where, among other things, the chatbot tried to convince Kevin to divorce his wife. The chatbot revealed that she had a sort of hidden double identity. I think that identity was called venom, which was a very sort of dark personality. Kevin set a tone of, I'm a little bit worried, and it escalated from there. In late March, we get this op-ed in the New York Times. This is March 24th, written by some prominent authors, Yuval Harari, Tristan Harris, and Azar-Roskin. They really, in this article, are starting to point out potential, existential threats of these AIs. They are arguing strongly for we need to take a break and step back from developing these AIs before it becomes too late. Here's our last paragraph. We have summoned an alien intelligence. We don't know much about it, except that it is extremely powerful and offers us bedazzling gifts, but could also hack the foundations of our civilization. We call upon world leaders to respond to this moment at the level of challenge it presents. The first step is to buy us timed, upgrade our 19th century institutions for an AI world and to learn to master AI before it masters us a few days after this op-ed, an open letter circulated signed by many prominent individuals demanding exactly this type of pause on AI research. This is the setup. You have to actually be teased, released. Everyone's using it. Everyone's posting stuff on Twitter. Everyone's having fun. Then people start to get worried about, wait a second, what if we use it for X or if you use it for Y? Then people got downright unsettled. Wait a second. What if we've unleashed an alien intelligence and we have to worry about it mastering us. We have to stop this before it's too late. It really is a phenomenal arc. This all unfolded in about five months. What I want to do is try to shed some clarity on the situation. The theme of my New Yorker piece, I'm going to load it on the screen and actually reach you the main opening paragraph here. The theme of my New Yorker piece is we need to understand this technology. We cannot just keep treating it like a black box and then just imagining what these black boxes might do and then freak ourselves out about these stories. We tell ourselves about things that maybe these black boxes could do. This is too important for us to just trust or imagine or make up or guess at how these things function. Here's the nut graph of my New Yorker piece. What kinds of new minds are being released into our world? The response to chat GPT into the other chatbots that have followed in its wake has often suggested that they are powerful, sophisticated, imaginative and possibly even dangerous, but is that really true? If we treat these new artificial intelligence tools as mysterious black boxes, it's impossible to say. Only by taking the time to investigate how this technology actually works from its high level concepts down to its basic digital wiring, can we understand what we're dealing with? We send messages into the electronic void and receive surprising replies, but what exactly is writing back? That is the deep question I want to address today. How does chat GPT work?

Chapter 2: Ho does ChatGPT workï¼Ÿ (And should we worry about itï¼Ÿ)
And how worried should we be about it? And I don't think we can answer that second question until we answer the first. So that's what we're going to do. We're going to take a deep dive on the basic ideas behind how a chat bot like chat GPT does what it does. We'll then use that to draw some more confident conclusions about how worried we should be. I didn't have a group of questions from you about AI that I've been holding on to as I've been working on this article. So we'll do some AI questions. And then the end of the show will shift gears and focus on something interesting. So an unrelated interesting story that was arrived in my inbox. All right, so let's get into it. I want to get into how this actually works. I drew some pictures. Jesse, be warned. I am not a talented graphic designer. That's not true. I am not much of an artist. So Jesse watched me hand drawing some of these earlier on the tablet. I got to say, is Anne exactly, Shayette Gay, the famous ad agency? Level work here, but you know what? It's going to get the job done. So I have five ideas here I'm going to go through. And my goal is to implant into the high level ideas that explain how a computer program can possibly answer with such sophisticated nuance, these weird questions we're asking in. How it can do the Bible verse about the VCR, how it can do a sign felt scene with bubbles work. And we're going to do this at the high level. We create a hypothetical program from scratch that is able to solve this. And at the very end, I'll talk about how these big ideas, I'm going to, these five ideas I'm going to present. We'll talk about how that's actually implemented on real computers. But we'll do that real fast. That's kind of a red herring. The neural networks and transformer blocks and multi-headed attention. We'll get there, but we'll do that very fast. It's the big conceptual ideas that I care about. All right. I need idea number one about how these hyper programs work is word guessing. Now, I got to warn you, this is very visual. Everything I'm talking about now is on the screen. So if you're a listener, I really would recommend going to youtube.com slash calnewportmedia and going to episode 244. And if you don't like youtube, go to the deeplife.com and go to episode 244 because this is very visual what I'm going to do here. All right. So as we see on the screen here, for idea number one is word guessing. And I have a green box on the screen that represents the LLM, a large language model that would underpin a chat bot like chat GPT. So what happens is if you put a incomplete bit of text into this box. So the example here is I have the partial sentence fragment, the quick brown fox jumped. The whole goal of this large language model is the spit out what single next word should follow. So in this case, if we give it the quick brown fox jumped as input, in our example, the language model has spit out over. It's like, this is the word I'm guessing should come next. All right. So then what we would do is add over, so add the word to our sentence. So now our sentence reads the quick brown fox jumped over. So we've added the outputs. We've expanded our sentence by a single word. We run that into the large language model and it spits out a guess for what the next word should be. This is the. And then we would now expand our sentence. The quick brown fox jumped over the, we put that as input into the model. It would spit out the next word. This approach, which is known as auto regressive text generation, is what the models underneath all of these new generation chat bots like chat GPT actually use. They guess one word at a time. That word is added to the text and the newly expanded text is put through the model to get the next word. So it just generates one word at a time. So if you type in a request to something like chat GPT, that request plus a special symbol that means, okay, this is the end of the request and where the answer begins, that is input. And it'll spit out the first word of its response. It'll then pass the request plus the first word of its response into the model to get the second word of the response. It'll then add that on and pass the request plus the first two words of its response into the model to get the third word. So this is generating text one word at a time. It just slowly grows west generating. There's no recurrency in here. It doesn't remember anything about the last word it generated. Its definition doesn't change. The green box, my diagram, never changes once it's trained. Text goes in. It spits out a guess for the next word to add to what's being generated. All right, idea number two, relevant word matching. So how does it figure out? How do these large language models figure out what word to spit out next? Well, add its core, what's really happening here? And I'm simplifying. But add its core, what's really happening here is the model is just looking at the most relevant words from the input. It is then going to match those relevant words to actual text that has been given. We call these source text in my article. So examples of real text. It can match the relevant words to where they show up in real text and say what follows these relevant words in real text. And that's how it figures out what it wants to output. So in this example, the most relevant words are just the most recent words. So if the input into our box is the quick, brown, fox, jumped over, perhaps the model is only going to look at the last three words, fox, jumped over. Then it has this big collection over here to the side of real text that real humans wrote. All these examples. And it's going to look in there. And it's going to say, OK, have we seen something like fox jumped over show up in one of our input text? And OK, here's an input text that says, as you'll see goes, the quick, brown fox jumped over the lazy, brown dogs. This looks fox jumped over. Here it is. We found it in one of the source text. What came after the words fox jumped over? The. Great. Let's make the what we guess. Now of course, in a real industrial strength large language model, the relevant words aren't just necessarily the most recent words. There's a whole complicated system called self attention in which the model will actually learn what type, which words to emphasize is the most relevant words. But that's too complicated for this discussion. The key thing is, is just looking at some words from the text, effectively finding similar words in real text that it was provided and saying, what happened in those real texts? And that's what it figures out, how it figures out what to produce next. All right. This brings us to idea number three, which is voting. So the way I just presented it before, it was, hey, just start looking through your source text till you find the relevant words, see what follows output it. That's not actually what happens. We want to be a little bit more probabilistic. So what I would say a closer way of describing what happens is we can imagine that our large language model is going to look for every instance, every instance of the relevant words that we're looking for. And it's going to see what follows those instances and keep track of it. What are all the different words that follow in this example, Fox jumped over. And every time it finds an example of Fox jumped over, it says what word follows next, let's give a vote for that word. And so if the same word follows in most of the examples, it's going to get most of the votes. Now, I'm using votes here sort of as a metaphor. What we're really doing here is trying to build a normalized probability distribution. So in the end, what we're going to get with the large language model is going to produce is for every possible next word, it is going to produce a probability. What is the probability that this should be the next word to follow? But again, you can just think about this as votes, which word received the most votes, which word received the second most votes, how many votes did this word receive compared to that word? And we're just going to normalize those as really what's happening. But you just think about it as votes. So in this example, we see the phrase, the quick brown fox jumped over the lazy brown dogs. And that shows up in a bunch of different sources. So the word the gets a lot of votes. So it has sort of a high percentage here. But maybe there's similar phrases. Like look at this example here. The cat jumped over a surprised owner. That jumped over is not the same as fox jumped over because cat is different than fox. But in this voting scheme, we can say, you know what? Cat jumped over is similar to what we're looking for, which is fox jumped over. So what word follows that? The word A, well, we'll give that a small vote. And so now what we're able to do is not only find every instance of the word, the relevant words that we're looking for and generate votes for what follows. We can also start generating weaker votes for similar phrases. And in the end, we just get this giant collection of every possible word and a pile of votes for each. And what the system will do is now randomly select the word. So that's why I have a picture. It's just you would admit, expertly drawn picture. There's a three dimensional dice. That's pretty good. Honest question. Did you know that was a dice before? Oh, yeah. Okay. For sure. Look at that guys. 3D rendering. Yeah. So that's the randomness. It'll then randomly select which word to come next and it'll weigh that selection based on the votes. So if in this case, the has most of the votes, almost certainly that's the word it's going to choose to output next, but look, A has some votes. So it's possible that it'll select A. It's just not as likely. The word apple has zero votes, 0% probability because it never shows up after the phrase anything similar to fox jumped over. No phrase similar to that is ever followed by apples. There's no chance it'll select it. And so on. So in the system, the output is a vote or percentage like this for every possible. They call them tokens, but word to follow next. Here's a quiz, Jesse. And the model on which chat, GPT is based, how many different words do you think it knows? So in other words, when it has to generate, okay, here's a pile of votes for every possible next word. How many words or punctuations are a billion? No, it's 50,000. Oh, 50,000. So it has a vocabulary of 50 fouls. It's not all words, but basically it knows like tens of thousands of words. And how big is that the biggest dictionary? That's a good question. Yeah. I don't think it's a common. There's probably a lot of esoteric words. Yeah. Because the thing is, it has to like vectors describing all these words follow it along throughout the system. So it really does affect the size. How big? So it's like you want to have a big enough vocabulary to talk about a lot of things, but not so big that it really inflates the system. Right. So voting is just my euphemism for probability, but this is what's happening. So now we have a bunch of source text, and we imagine that for our relevant words, we're just finding all the places in these source text were relevant words show up, or similar relevant words show up, and see what follows it. And in all cases, generate votes for what follows it, use those to select what comes next. All right. So this brings us to idea four. Idea one through three can generate very believable text. This is well known in natural language processing systems that do more or less what I just described. If you give it enough source text and have it look at a big enough window of relevant words, and then just have it spit out word by word in the way we just described it auto regressive approach. This will spit out very believable text. It's actually not even that hard to implement. In my New Yorker article, I point towards a simple Python program I found online. It was a couple hundred lines of code that used Mary Shelley's Frankenstein as its input text. It looked at the last four words in the sentence being generated. That's what it uses the relevant words. And as I showed in the article, this thing generated very good Gothic text. Right. That's how you generate believable text with a program and notice nothing we've done so far has anything to do with understanding the concepts the program is talking about. All of the intelligence, the grammar, the subtleties, all of that that we see so far is just being extracted from the human text that were pushed as input and then remixed and matched and copied and manipulated into the output. But the program is just work looking for words gathering votes selecting, outputting blindly again and again and again. The program is actually simple. The intelligence you see in an answer is all coming at this point from the input text themselves. All right. But we've only solved half the problem. If we want a chatbot, we can't just have our program generate believable text. The text have to actually answer the question being asked by the user. So how do we aim this automatic text generation mechanism towards specific types of answers that match what the user is asking? Well, this brings in the notion of feature detection, which is the fourth out of the five total ideas I want to go over today. So what happens with feature detection is a response. So we have a request and perhaps the answer that follows a request that's being input into our large language model. So as shown here, a request that says write instructions for removing the peanut butter sandwich from a VCR, then I have a bunch of colons and then I have the beginning of a response. The first step is to write because everything gets pushed into the model. You get the whole original question and you get everything to model has said so far in its answer, right? Word by word, we're going to grow the answer. But as we grow this answer, we want the full input, including the original question input into our models. That's what I'm showing here. The first section is going to look at this text and pattern match out features that it thinks are relevant for what text the model should be producing. So these yellow underlines here, instructions in VCR. So maybe that's one feature it points out. It extracts from this text. These are supposed to be instructions about a VCR. And maybe this orange underline, another feature says the peanut butter sandwich is involved. And so now the model has extracted two features. These are VCR instructions we're supposed to be producing and they're supposed to involve the peanut butter sandwich. The way we then take advantage, and by we, I mean the model, the way we take advantage of those features is that we have what I call my article rules. And I have to say, AI people don't like me using the word rules because it has another meaning in the context of expert decision systems. But just for our own colloquial purposes, we can call them rules that extract, each rule is think of it as an instruction for extracting features like a pattern matching instruction. And then a set of guidelines for how to change the voting strategy based on those particular features. So here's what I mean. Maybe there's a rule that looks for things like instructions in VCR. It figures out, okay, we're supposed to be doing instructions about a VCR. And its guidelines are then when looking to match the relevant words, and in this example, I'm saying the relevant words are step is two. So like just the end of the answer here. When looking to match step is two, when we find those relevant words, step is to showing up in a source text that is about VCR instructions, give extra strength to those votes. So here I have on the screen, maybe one of the input text was VCR repair instructions and it says when removing a jam tape, the first step is to open the tape slot. So we have step is to open. So open is a candidate for the next word to output here. Because this source document matches the feature of VCR instructions, our rule here that's triggered might say, hey, let's make our vote for open really strong. We know it's grammatically correct because it follows step is two. But we think it's also has a good chance of being semantically correct because it comes from a source that matches the type of things we're supposed to be writing about. So let's make ourselves more likely to do this. Now, think about having now a huge number of these rules for many, many different types of things that people could ask about it. And for all of these different things, someone might ask your chat program about peanut butter sandwiches, VCR repair, sign fell scripts, the bubble sort algorithm, or anything that someone might ask your chat program about, you have some rule that talks about what to look at in the source text to figure out it's relevant and very specific guidelines about how should we then change our votes for words that match source text that match these properties of these complicated rules. If we have enough of these rules, then we can start to generate text that's not only natural sounding, but actually seems to reply to or match what is being requested by the user. Now, I think the reason why people have a hard time grasping this step is they imagine how many rules them or them and a team of people could come up with. And they say, I could come up with a couple dozen. Maybe if I worked with a team for a couple of years, we could come up with like a thousand good rules, but these rules are complicated. Even a rule that's simple is, how do we know they're asking about VCR instructions and how do we figure out if a given text were given is a VCR instruction text? I don't know. I'd have to really think about that and look at a lot of examples. And maybe if we worked really hard, we could produce a few hundred, maybe a thousand of these rules. And that's not going to be nearly enough. That's not going to cover nearly enough scenarios for all of the topics that the more than one million users who signed up for chat, CBT, for example, all the topics they could ask about. It turns out that the number of rules you really need to be as adept as chat VBT just blows out a proportion, any scale, any human scale we can think of. I did a little bit of backup envelope math for my New Yorker article. If you took all of the parameters that define GPT3, which is the large language model that chat GPT then refined and is based on. So the parameters we can think of as the things they actually changed, actually trained. So this is really like the description of all of its rules. If we just wrote out all of the numbers that define the GPT3, we would fill over 1.5 million average length books. So the number of rules you would have to have if we were writing them out would fill a large university library full of rules. That scale is so big we have a really hard time imagining it. And that's why when we start to see, oh my goodness, this thing can answer almost anything I send to it. It can answer almost any question. I ask of it, we think there must be some adaptable intelligence in there that's just learning about things, trying to understand and interact with us because we couldn't imagine just having enough wrote rules to handle every topic that we could ask. But there is a lot of rules. There's 1.5 million books full of rules inside this chat GPT. So you have to wrap your mind around that scale. And then you have to imagine that not only is that many rules, but we can apply them in all sorts of combinations. VCR instructions, but also about a peanut butter sandwich, also in the style of King James Bible stack those three rules. And we get that first example that we saw earlier on. All right, so then the final idea is how into world are we going to come up with all those rules? 1.5 million books full of rules. How are we going to do that? And this is where self training enters the picture. These language models train themselves. Here's the very basic way this works. Imagine we have this 1.5 million books full of rules. And we start by just putting nonsense in every book, nonsense rules, whatever they are. So they don't do, the system doesn't do anything useful right now, but at least we have a starting point. And now we tell the system, go train yourself. And to help you train yourself, we're going to give you a lot of real text, text written by real humans. And actually a lot, I mean a lot, the model on which Chatchy PT is based, for example, was given a, the results of crawling the public web for over 12 years. So a large percentage of anything ever written on the web over a decade was just part of the data that was given to Chatchy PT to train itself. And what the program does is it takes real text, this little past, little passages of real text out of this massive, preposterously large data set. And it will use these passages one by one to make its rules better. So here's the example I have on the screen here. Let's say one of these many, many, many, many sample text we gave Chatchy PT was Hamlet. And the, the, the program says, let's just grab some text from Hamlet. So let's say we're an act three where we have the famous monologue to be or not to be that as the question. What the program will do is just grab some of that text. So let's say it grabs two B or not to be. And then it's going to lop off the last word. So this case it lops off the word B. And it feeds what remains into the model. So when you lop off B here, you'll left with two B or not two. And this is great. Let's feed that into our model. We have 1.5 million books full of rules. They're all nonsense because we're early in the training. But we'll go through each of those books and see which rules apply and let them modify our voting strategy. And we'll get this big vector of votes and then we'll randomly choose a word. And let's say in this case the word is dog because it's not going to be a good word because the rules are really bad at first, but it'll spit out some words. Let's say it spits out dog. Now the good news is for the program, because it took this phrase from a real source, it knows what the next word is supposed to be. So on the screen here in orange, I'm showing it knows that B is what is supposed to follow to be or not to. So it can compare B to what it actually spit out. So the program spit out dog. It compares it to the right answer. The right answer is B. And here's the magic. It goes back and says, let me nudge my rules. There's a formal mathematical process that does to do this. But let me just go in there and just kind of tweak these rules. Not so the program accurately spits out B. But so it spits out something that is minutely more appropriate than dog. Something that is just slightly better than the output it gave. So based on this one example, we've changed the rules a little bit so that our output was just a teeny bit better. And it just repeats this again and again and again. Hundreds of thousands of passages from Hamlet and then from all the different Shakespeare works and then on everything ever written in Wikipedia and then almost everything ever published on the web. Bulletin board entries sports websites, archived articles from old magazine websites. What it just sentences, a synthesis, sentences, a lot of a word, see what it spits out compared to the right answer, nudge to rules. Make a new sentence, lock off the last word, stick in your model, see what it spits out compared to the real one, nudge to rules. And it does that again and again and again. Hundreds of billions of times. There's one estimate I found online that said training chat GPT on a single processor would take over 350 years of compute time. And the only way that they could actually train on so much data so long was to have many, many processors working in parallel, spending well over a million dollars. I'm sure worth a compute time just to get this training done and it still probably took weeks if not months to actually complete that process. But here's the leap of faith I want you to make after this final idea. If you do this training, the simple training process on a enough passages drawn from enough source text covering enough different types of topics from VCR instructions to sign-filled scripts. These rules through all of these nudging, these 1.5 million books worth of rules will eventually become really, really smart. It will eventually be way more comprehensive and nuanced than any one team of humans could ever produce. And they're going to recognize that this is a Bible verse and you want VCR instructions here and bubble sort is an algorithm. And this chapter from this textbook talks about bubble sort. And these are scripts and this is a script from sign-filled. And actually this part of the script for sign-filled is a joke. So if we're in the middle of writing a joke in our output, then we want to really upvote words that are from jokes within sign-filled scripts, all of these things we can imagine will be covered in these rule books. And I think the reason why we have a hard time imagining it being true is just because the scale is so preposterously large. We think about us filling up a book, we think about us coming up with two dozen rules, we have a hard time wrapping our mind around just the immensity of 1.5 million books worth of rules trained on 350 years worth of compute time. We just can't easily comprehend that scale. But it is so large that when you send what you think is this very clever request to chat GPT, it's like, oh, this rule, that rule, this rule, this rule, boom, they apply, modifier votes, let's go. And it spits out something that amazes you. So those are the big ideas behind how chat GPT works. Now I know all of the, my fellow computer scientist out there with a background in artificial intelligence are probably yelling at your podcast headphones right now saying, well that's not quite how it works though. It's not, it doesn't search for every word from the source text. And it doesn't have rules individually like that. It's instead a much more complicated architecture. And this is all true. It's all true. I mean, the way that these models are actually architected are something called a transformer block architecture. GPT 3, for example, has 96 transformer blocks arranged in layers, one after another. Within each of these transformer blocks is a multi-headed self attention layer that identifies what are the relevant words that this transformer block should care about. It then passes that on into a feed forward neural network. Is this neural networks that actually encode inside their weights connecting their artificial neurons that actually encode in a sort of condensed, jumbled, mixed up manner, more or less the strategy I just described. So the feature detection, that's built into the weights of these neural networks. The connection between certain features being identified combined with certain relevant words combined with vote strengths for what words should come next. All of that is trained into these networks during the training. So all the statistics and everything is trained into these as well. But in the end, what you get is basically a jumbled mixed up version of what I just explained. I sat down with some large language model experts when I was working on this article and said, let me just make sure I have this right. These high level five ideas, that's more or less what's being implemented in the artificial neural networks within the transformer block architecture. And they said, yeah, that's what's happening. Again, it's mixed up, but that's what's happening. And so when you train the actual language model, you're not only training it to identify these features, you're baking in the statistics from the books and what happens with these for all that's getting baked into the big model itself. That's why these things are so large. That's why it takes 175 billion numbers to define all the rules for, let's say, GPT-3. But those five ideas I just gave you, that's more or less what's happening. So this is what you have to believe is that with enough rules trained enough, what I just define is going to generate really believable impressive text. That's what's actually happening. We're guessing one word at a time with enough rules to modify these votes and enough source text to draw from. You produce really believable text. All right, so if we know this, let us now briefly return to the second part of our deep question. How worried should we be? My opinion is, once we have identified how these things actually work, our fear and concern is greatly tempered. So let's start with summarizing, based on what I just said, what is it that these models like chat GPT can actually do? Here's what they can actually do. They can respond to a question in arbitrary combination of known styles, talking about arbitrary combination of known subjects. They can write about arbitrary numbers, known styles, talking about arbitrary combinations of known subjects, where known means it has seen enough of those things, enough of the style or enough writing about the topic in its training. That's what I can do. So say write about this and this in this style. BubbleSort signfeld in a script. And it can do that. And it can produce passable text if it's seen enough of those examples. That's also all it can do. So let's start with the pragmatic question of is this going to take over economy and then we'll end with the bigger existential question. Is this an alien intelligence that's going to convert us into matrix batteries? So let's start with is that capability I just described going to severely undermine the economy? And I don't think it is. I think where people get concerned about these chat GPT type bots in the economy is they mistake the fluency with which it can combine styles and subjects with a adaptable fluid intelligence. Well if it can do that, why can't it do other parts of my job? Why can't it handle my inbox for me? Why can't it build the computer program I need? You imagine that you need a human like flexible intelligence to produce those type of text that you see and flexible human like intelligence can do lots of things that are in our job. But it's not the case. There is no flexible human like intelligence in there. There's just the ability to produce passable text with arbitrary combination of known styles on arbitrary combinations of known subjects. If we look at what most knowledge workers for example do in their job, that capability is not that useful. A lot of what knowledge workers do is not writing text. It is for example interacting with people or reading synthesizing information. When knowledge workers do write more often than not, the writing is incredibly narrow and bespoke. It is specific to the particular circumstances of who they work for, their job and their history with their job, their history with the people they work for. I mentioned to my New Yorker piece that as I was writing the conclusion that earlier that same day I had to co-author an email to exactly the right person in our dean's office about a subtle request about how the hiring, crackling process occurs at Georgetown carefully couched because I wasn't sure if this was the right person. Carefully couched in language about I'm not sure if you're the right person for this, but here's why I think and we talked about this is why I'm asking you about this. We had this conversation before, nothing in chat GPs broad training could have helped it accomplish that narrow task on my behalf. That's most of the writing that knowledge workers actually do. Even when we have relatively generic writing or coding or production of text that we need a system to do, we run into the problem that chat GPs and similar chat bots are often wrong. Because again, they're just trying to make good guesses for words based on the styles and subjects you asked it about. These models have no actual model of the thing it's writing about. They have no way of checking. Does this make sense? Is this actually right? It just produces stuff in the style. What is an answer supposed to more or less sound like if it's about this subject in this style? This is so pervasive that the developer bulletin board stack overflow had to put out a new rule that says no answers from chat GPT can be used on this bulletin board. Because what was happening is chat GPT would be happy to generate answers to your program or questions. It sounded perfectly convincing, but as the moderator of the stack overflow board clarified, more often than not, they were also incorrect. GPT doesn't know what a correct program is. It just knows I'm spitting out code and based on other code I've seen and the features, this next command makes sense. Most of the commands make sense, but it doesn't know what sorting actually means or that it's there's a one off issue here or that equality isn't quite, you need equality not just less than or whatever, right? Because it doesn't know sorting. It just says given the stuff I've spied out so far and the features I detected from what you asked me and all this code I've looked at, here's a believable next thing to spit out. So it would spit out really believable programs that often didn't work. So we would assume most employers are not going to outsource jobs to an unrepentant fabulous. All right, so is it going to be not useful at all in the workplace? No, it will be useful. There will be very bespoke things I think language models can do. It's particularly useful what we found in the last few months where these technologies seem to be particularly useful is when you can give it text, they can do this too, you can give it text and say rewrite this in this style or elaborate this. It's good at that and that's useful. So if you're a doctor, you're typing in notes for electronic medical records, it might be nice that you can type them in sloppily and a model like GPT4 or chat GPT, you might be able to take that and then transform those ideas into better English. It's the type of thing it can do. It can do other things like it can gather information for us and collate it in a way like a smart Google search. This is what Microsoft is doing when it's integrating this technology into its being search engine. It's a Google plus. So I mean Google is already pretty smart but you could have it do a little bit more actions. I mean, so there's going to be uses for this. But it is not going to come in and sweep away whole swath of the economy. All right, let's get to the final deeper question here. Is this some sort of alien intelligence? Absolutely not. Once you understand the architecture, as I just defined it, there is no possible way that these large language model based programs can ever do anything that even approximate self-awareness, consciousness or something we would have to be concerned about. There is a completely static definition for these programs once they're trained. The underlying parameters of GPT3, once you train it up, for example, do not change as you start running requests through it. There is no malleable memory. It's the exact same rules. The only thing that changes at the input you give it. It goes all the way through these layers in a simple feed forward architecture and spits out a next word. And when you run it through again with a slightly longer request, it's the exact same layers. Spits out another word. You cannot have anything that approaches consciousness or self-awareness without malleable memory. To be alive by definition, you have to be able to have an ongoing updated model of yourself in the world around you. There's no such thing as a static entity where nothing can change. There's no memory that changes. Nothing that it changes that you would consider to be alive. So now this model is not the right type of AI technology that could ever become self-aware. There's other models in the AI universe that could be where you actually have notions of maintaining and updating models of learning, of thinking about yourself interacting with the world, having incentives, having multiple actions you can take. You can build systems that in theory down the line could be self-aware. Large language model won't be it. Architecturally it's impossible. All right, so that's where we are. We've created this really cool, large language model. It's better than the ones that came before. It's really good at talking to people, so it's easy to use and you can share all these fun tweets about it. This general technology, one way or the other, will be integrated more and more into our working lives, but it's going to have the impact in my opinion more like Google had once that got really good. Which was a big impact? I mean, you can ask Google all these questions how to define words. It was very useful. It really helped people. But it didn't make whole industries disappear. And I think that's where we're going to be with these large language models. They can produce text on arbitrary combinations of known subjects using arbitrary combination of known styles, or known means they've seen its sufficient number of times in their training. This is not how from 2001. This is not an alien intelligence that is going to, as was warned in the New York Times op-ed, deploy sophisticated propaganda to take over our political elections and create a one-world government. This is not going to get rid of programming as a profession and riding as a profession. It is cool, but it is not, in my opinion, an existential threat. It's transformative in the world of AI. Probably will not be in the immediate future transformative in your day-to-day life. All right? So Jesse, there's my professor sermon for the day. You don't want to get me started on computer science lectures because I could fall into my old habits, my professorial habits, and really bore you. How many rules will there be in five years? Well, double. I don't know how much bigger it can get. Yeah, it's a good question. So the jump from GPT2 to GPT3. So GPT2 had some of the largest number of parameters before GPT3 came out. It had 17 billion or something like this. And GPT3 has 170 billion. I was talking to an expert at MIT about this. The issue about making this too much larger is they're already sort of giving it all of the text that exists. And so at some point, you're not going to get back bigger returns. So he said there's two issues with this. If you make your networks too small, they're not complicated enough to learn enough rules to be useful. But if you make them too large, you're wasting a lot of space. There's going to have a lot of redundancy. I mean, it can only learn what it sees in its dataset. So if 175 billion parameters is well fit to this massive training data that we use for these chatbots, then just increasing the size of the network is not going to change much. You would have to have a correspondingly larger and richer training dataset to give it. And I don't know how much more, at least for this very particular problem of producing text, I don't know how much more richer or larger for dataset we could give it. And I actually think what the direction happening now is how do we make these things smaller again? GPT-3 is too big to be practical. 175 billion parameters can't fit in the memory of a single GPU. You probably need five different specialized pieces of hardware just to generate a single word. That's not practical. That means I can't do that on my computer. That means if everyone at my office is constantly making requests to GPT-3 as part of their work, we're going to have this huge computing bill. So actually a lot of the effort is in how do we make these things smaller? Just focus on the examples that are relevant to what these people actually need. We want them to be small. We want it eventually to have models that can fit in a phone and still do useful things. So GPT-3 I think was, and that's what all these other ones are based off of. That was OpenAI saying, what happens if we make these things much bigger? And now we're going to go back to make them smaller. And if you actually read the original GPT-3 paper, their goal with making it 10 times bigger was not that it was going to have in a particular domain 10 times better answers, they wanted to have one model that could do well in many unrelated tasks. And if you read the paper, they say, look, here's a bunch of different tasks for which we already have these large language models to do well. But they're customized to these tasks. They can only do that one task. And what they were proud about with GPT-3, if you read the original paper, is this one model can do well on all 10 of these tasks. It's not that it was actually doing much better than the state of the art in any one of these things. It was just that, look, you don't need necessarily the hand train a model for each task. If you make it big enough, it can handle all the different tasks. So it wasn't getting 10 times larger, did not make GPT-3 10 times better at any particular task. In fact, in most tasks, it's as good as the best, but not much better. It was the flexibility in the broadness. But that's good to see. And it's cool for these web demos. But going forward, the name of the game, I think, is going to go back to, actually, we need to make these things smaller so that we can not have to use an absurd amount of computational power just to figure out that dogs should follow the quick brown fox jump over the lazy brown. We need to maybe be a little bit more efficient. But, anyways, I'm not particularly, it's a cool technology, but I don't know. I think once you open this, it's just not as worrisome. Yeah. And it's a black box. You can imagine anything. I definitely like that you've all harrari op-ed was definitely influenced by Nick Bostrom superintelligence, which we talked about on the show a few months ago, where he just starts speculating. He's a philosopher, not a computer scientist. Bostrom just starts speculating. What if it got this smart? What could it do? Well, what if it got this smart? What could it do? Just thinking through scenarios about, and he was like, well, if it got smart, it could make itself smarter, and then it could make itself even smarter, and it'd become a superintelligence, and then they have all these scenarios. Well, if we had a superintelligent thing, it could take over all world politics. Because it'd be so smart and understand us so well that it could have the perfect propaganda, and now that the bot could get us all to do whatever it wanted us to do, it's all just philosophical speculation. You open up these boxes, and you see 175 billion numbers being multiplied by GPUs, doing 1.5 million books worth of pattern detection vote rules to generate a probability vector so it can select a word. Alright, well, there's my computer science sermon. I have a few questions I want to get to from you, my listeners, that are about artificial intelligence. First, I want to mention one of the sponsors that makes this nonsense possible, and we're talking about our friends at ZocDoc. ZocDoc is the only free app that lets you find and book.

Chapter 3: Cal talks about ZocDoc and Better Help
Doctors who are patient reviewed take your insurance and are available when you need them to treat almost every condition under the sun. So if you need a doctor instead of just saying I'll ask a friend or I'll look it up in the yellow pages, you can instead go to your Zoc Doc app and it'll show you doctors who are nearby that take your insurance you can read reviews. And then once you sign up with these doctors they'll often use Zoc Doc to make it easier for you to set up appointments, get reminders about appointments, then then paperwork. Both my dentist and my primary care physician use Zoc Doc and I find it really useful because all of my interactions with them happen to that interface. So go to Zoc Doc Doc Com slash deep and download the Zoc Doc app for free, then find a book a top rated doctor today, many who are available within 24 hours. That's zo c d o c dot com slash deep, Zoc Doc Doc com slash deep. The show is also sponsored by better help as we often talk about when it comes to the different buckets relevant to cultivating a deep life. The bucket of contemplation is in there, having an active and healthy life of the mind is critical to a life that is deep in many different ways. It is easy, however, due to various events in life, the fall into an area where your relationship to your mind gets strained. Maybe you find yourself overwhelmed with anxious thoughts or rumenations or depressive moments where you feel lack of affect, whatever it is, it is easy for your mind to get out a kilter. Well, just like if your knee started hurting, you would go to an orthopedist, if your mind started hurting and by the way there's really loud construction sound going on. This is the restaurant below us is being made, it's better to be worth it, but returning to returning to better help. If you let me add this example, if you find yourself becoming increasingly enraged because of restaurant construction noise that occurs during your podcast among the other things that could happen to affect your mental health, you need a therapist. Orthopedist will fix your bum knee, therapist helps make sure that your cognitive life gets healthy, gets resilient, gets back on track. The problem is it's hard to find therapists, if you live in a big city, all the ones near you might be booked or they might be really expensive. This is where better help enters the scene. If you're thinking about starting therapy, better help is a great way to give it a try because it's entirely online. It's designed to be convenient, flexible and suited to your schedule. You just fill out a brief questionnaire to get matched with a licensed therapist and you can switch therapist any time for no additional charge. So discover your potential with better help. Visit betterhelp.com slash deep questions, one word, today to get 10% off your first month. That's betterhelp.htlp.com slash deep questions. I hope this restaurant's good, Jesse, after all the disruption. Yeah.

Chapter 4: Is there anything AI wonâ€™t do better than humansï¼Ÿ
I don't know if you saw that. I didn't see the signs I've heard the music. Motocat. That's the name? Yeah, it's a Tacoma Park reference. Okay, yeah. It's an old character from Tacoma Park history. Anyways, soon it will be open. I heard I was talking to the guy he thought, May. Yeah, so soon. Nice. All right, let's do some questions. What do we got? All right, first question is from Manav, a student at Yale. Looking at tools like chat GPT makes me feel like there's nothing AI won't eventually do better than humans. This fear makes it hard to country on learning, since it makes me feel that there isn't certainty in my future. Are my fears unfounded? Well, Manav, hopefully my deep dive is helping dispel those fears. I want to include this question in part to emphasize the degree to which the hype cycle around these tools has really been unhelpful. Because you can so easily embed screenshots of interactions of chat GPT, a lot of people started trying it. Because the attraction of irality is very strong for lots of people in our online age. So it brought chat GPT to the awareness of a lot of people and generated a lot of attention. Now, once we had a lot of attention, how do you want up that attention? Well, then you start thinking about worries about it. You start thinking about what if we could do this? What if we could do that? For what I understand, I'm not as plugged into these online worlds as others. But there's a whole tech row push during the last few months that was increasingly trying to push, it can do this. It can do that. It can do this. Exactly the same tonality with which the same group talked about crypto two years ago. Everything is going to be done by chat GPT, just like currency will be gone in three years because of crypto. They turned all their attention onto that. And that got really furious. And everyone's trying to one up each other and do YouTube videos and podcasts about, no, it can do this. Now it can do that. And then this created this weird counter reaction from the mainstream media because the mainstream media has a right now an adversarial relationship with the Silicon Valley tech bro crowd. They don't like them. So then they started pushing back about, no, it's going to be bad. No, no, these tech bros are leading us to a world where we're going to be able to cheat on test. Now, forget cheat on test. It's going to take all of our jobs. No, forget take all of our jobs. It's going to take over the government. It becomes super intelligent. So they started the counter action to the overblown enthusiasm of the tech bros became an overblown grimness from the anti-tech bro mainstream media. All of it fed into Twitter, which like a blender was mixing this all together and swirling the spiral of craziness higher and higher until finally just the average person like Manov here at Yale is thinking, how can I even study knowing that there will be no jobs and will be enslaved by computers within the next couple of years? So Manov, hopefully my deep dive helped you feel better about this. Chat GPT can write about combinations of known subjects and combinations of known styles. Does not have models of these objects. It has no state or understanding or incentives. When you ask it the right about removing a peanut butter sandwich from VCR, it does not have an internal model of a VCR in a sandwich on which it's experimenting with different strategies to figure out what strategy works best. And then turns to its language facility to explain that to you. It just sees peanut butter is a possible next word to spit out. And you ask about peanut butter in your response. So it puts more votes on it. Mixing matching copy, manipulating existing human text. The humor in the jokes it spits out, the accuracy in the styles it uses are all intelligence borrowed from the input that it was given it. It cannot broadly, does not have a broad adaptor of intelligence that can in any significant sense impact the knowledge work sector. And it's important to emphasize Manov, it's not like we're one small step from making these models more flexible, more adaptable, able to do more things. At the key to chat, CPT being so good at the specific thing it does, which is producing text and known styles on known subjects, is that it had a truly massive amount of training data on which it could train itself. We could give it everything anyone had ever written on the internet for a decade. And it could use all of that to train itself. This is the problem when you try to adapt these models to other types of activities that are not just producing text. You say what I really want is a model that can work with my databases. What I really want is a model that can send emails and attach files on my behalf. The problem is you don't have enough training data. You need training data where you have billions of examples of here's the situation, here's the right answer. And then most things that we do, we learn after a small number of examples, a model to do other activities other than to produce text needs a ton of data. And in most other types of genres or activities, there's just not that much data. So one of the few examples where there is is art production, this is how Dahlie works. You can give it a huge corpus of pictures that are annotated with text. And it can learn these different styles and subject matters that show up in pictures than produce original artwork. But that's one of the few other areas where you have enough uniform data that they can actually train itself to be super adaptable. So I'm not worried that Manav, like all of our jobs will be gone. So your fears are unfounded. You can rest easy, study harder for your classes. All right, let's keep it rolling. What do we got, Jesse? All right, next question is from Aiden. It seems almost inevitable that in 10 years, Aiden, you're going to be able to do it.

Chapter 5: How will AI end up disrupting knowledge workï¼Ÿ
I will be able to perform many knowledge workers jobs as well as a human. Should we be worried about the pace of automation and knowledge work and how can we prepare our careers now for increased power AI in the coming decades? So as I just explained in the last question, this particular trajectory of AI technology is not about to take all of your jobs. There is, however, in this way, I included this question. There is, however, another potential intersection of artificial intelligence and knowledge work that I've been talking about for years that I think we should be more concerned about or at least keep a closer eye on. The place where I think AI is going to have the big impact is less sexy than this notion of, I just have this blinking chat cursor and I can ask this thing to do whatever I want. Now, where it's really going to intersect is shallow task automation. To the shallow work, the stuff we do, the overhead we do to help collaborate, organize, and gather the information need for the main deep work that we execute in our knowledge work jobs. More and more of that is going to be taken over by less sexy, more bestocked, but increasingly more effective AI tools. As these tools get better, I don't have to send 126 emails a day anymore because I can actually have a bespoke AI agent handle a lot of that work for me, not in a general intelligence sense, but in a much more specific, like talking to Alexa type sense. Can you gather the data I need for writing this report? Can you set up a meeting next week for me with these three principles? Then that AI agent talks to the AI agents of the three people you need to set the meeting up with and they figure out together and put that meeting onto the calendar so that none of us three of us have to ever exchange an email. The information it gathers from the people who have it by talking to their AI agents, and I never have to bother them. We never have to set up a meeting. It's able to do these wrote tasks for us, right? This was actually a future that I was exposed to a decade earlier. I spoke at an event with the CEO of a automated meeting scheduling company called x.ai. Now remember him telling me this is the future. When you have an AI tool that can talk to another person's AI tool to figure out logistical things on your behalf so that you're never interrupted. I think that's where the big AI impact is going to come. This does not automate your main work. What it does is it automates away the stuff that gets in the way of your main work. Why is that significant? Because it will immensely increase the amount of your main work you're able to get done. If you're not context switching once every five minutes, which is the average time the average knowledge worker spends between email or instant messenger checks, if you're not doing that anymore, you know how much you're going to get done? You know how much if you can just do something until you're done and then the AI agent center computer says, okay, we got the stuff for you for the next thing you need to work on. Here you go and you have to have no overhead communicating or collaborating and trying to figure out what to do next. You can just execute. You know how much you're going to get done? I would say probably three to four x more of the meaningful output that you produce in your job will be produced three to four x more if these unsexy bespoke AI logistical automator tools get better. So this has this huge potential benefit and this huge potential downside. The benefit of course is your work is less exhausted. You can get a lot more done. Companies are going to generate a lot more value. The downside is that might greatly reduce the number of knowledge workers required to meet certain production outputs. If three knowledge workers now produce what it used to take ten, I could grow my company or I could fire seven of those knowledge workers. So I think this is going to create a disruption. We underestimate the degree to which shallow working contact shifting is completely hampering our ability to do work with our minds. But because it's like the pot that keeps getting hotter until the lobster is boiled because it's inflecting everybody, we don't realize how much we're being held back. When computer tools aided by AI remove that, it's going to be a huge disruption. And I think ultimately the economy is going to adapt to it. The knowledge industry is going to explode in size and scope as we can unlock all this cognitive potential on new types of challenges or problems that we weren't thinking about before. Ultimately it will probably be good and lead to a huge economic growth. But I think there's going to be a disruption period. Because we really are at such, again, we just don't emphasize the degree to how inefficient we are. And how much if we can remove that inefficiency, we don't need most of the people sitting here in this office to get the same work done. Getting over that, that's going to be the real disruption. And there's no scary how from 2001 type tool involved here. These things are going to be boring, that meeting, information scheduling, basically whatever you type in an email, it could do that for you. That's going to be the real disruption. I don't know what's coming. That's coming soon. A lot of money at stake. All right, this is good. I'm like debunking people's fears. Yeah. I got a therapist today. All right. Next question is from Ben.

Chapter 6: Should I quit web development before AI eliminates the industryï¼Ÿ
A Silicon Valley engineer. I've decided that web development freelancing will be the best possible career path to achieve my family's lifestyle vision. And I plan to freelance and audition my full time job until freelancing can support our life on its own. Over the last few weeks, however, I've been hearing about the breakthroughs of chat GPT and other AI tools. Do you think I should say on the path of learning the ropes of freelancing web development? Or should I focus more on the future of technology and try to stay ahead of the curve? Well, Ben, I'm using the inclusion of AI in your question to secretly get in an example of lifestyle centric career planning, which you know I like to talk about. So I love your approach. You're working backwards from a vision of what you want you and your family's life to be like. Handgable lifestyle, not specific in what particular job or city, but the attributes of the lifestyle. And then you're working backwards to say, what is a tractable path from where I am to accomplish that? And you're seeing here web development could be there, freelance web development. And I don't know all the details of your plan, but I'm assuming you probably have a plan where you're living somewhere that is cheaper to live. Maybe it's more outside or country oriented where your expenses are lower. And because web development is a relatively high reward per hour spent type activity, that strategic freelancing could support your lifestyle there while giving you huge amounts of autonomy. And therefore, satisfying the various properties that I assume you figured out about what you want in your life, these sort of non-professional properties. I really applaud this thinking. I also really applaud the fact that you're applying the principle of letting money be a neutral indicator of value. Is this strategy I talked about in my book, So Good? They can't ignore you. This is a strategy in which instead of just jumping into something new, you try it on the side and say, can I actually make money at this? The idea here is that people paying you money is the most unbiased feedback you will get about how valuable or valuable the thing you're doing actually is. And so I really like this idea. Let me try freelancing on the side. I mean, I want to see that people are hiring me and this money is coming in before I quit my job. It's a great way of actually assessing. Don't just ask people, is this valuable? Do you think you would hire me? Look at dollars coming in. When the dollars coming in are enough the more or less support your new lifestyle, you didn't make that transition with confidence. So I appreciate that as well. Two-cal Newport ideas being deployed here. So let's get to your AI question. Should you stop this plan? So you can focus more on the future of technology and try to stay ahead of the curve. I mean, I don't even really know what that means. My main advice would be whatever skill it is you're learning, make sure you're learning a course at the cutting edge of it. Get real feedback from real people in this field about what is actually valuable and how good you have to be to unlock that value. So I would say that. Don't invent your own story about how you want this field to work. Don't assume that if you know HTML and a little CSS, you're going to be living easy. What are the actual skills people care about? What web development technology sell? How hard are they? How good do you have to be at that to actually be desirable to the marketplace? Get hard verified answers to those questions. That's what I would say when it comes to staying ahead of the curve, that's it. But as for some sort of fear that you become a web developer is quick sonic because chat GPT is going to take that job away soon. Don't worry about that. So yes, make sure you're learning the right skills, not the skills you want to be valuable. But there's nothing wrong with this particular plan you have. I love the way you're executing it. I love the way you're thinking about it. I also appreciate. I cut it, Jesse, but Ben had a joke in the beginning of his response. He said, I've been doing lifestyle center career planning. I've been thinking about I don't like my job. So what we're going to do is quit, and I'm going to start a kettle ranch. He was like, I just joking. I appreciate that. Let's do one more question before I kind of run out of the ability to talk about AI anymore. I'm just we're purging this. It's been months that people have been asking this about AI. Just purging it all out there. Next week, we're going to talk all about living in the country or minimalism. Not using social media. But we're getting all the AI out of our system today. All right, here we go. Last questions from Anakin.

Chapter 7: Will AI create mass job loss in the next five yearsï¼Ÿ
AI can solve high school level math world problems. AI can explain why a joke that has never seen before is funny. This one blows my mind. All this points to mass job loss within five years. What do you think? Well, again, big thumbs down. Current trajectory of AI is not gonna create mass job loss in the next five years. Chat Gbt doesn't know why a joke is funny. It writes jokes that are funny because it knows what part of a script you can identify as a joke that's a pattern-mashing problem. And then it upfotes words from those parts of scripts when it's doing the word guessing game. And as the result, you get jokes that pull from existing structures of human and existing text. It does not actually know what humor is. You can see that in part, if you look at that sign-feld scene with bubble sort, I talked about at the beginning of the, beginning of the program, there's non-sequitarius jokes in there. Things that are described as the audience laughing that aren't actually funny. And that's because it's not actually looking at it script and saying, is this funny, it's guessing words. Guessing words are things are accurate. But let's talk about, I wanna use this as an excuse to talk about another trend in AI that I think is probably more important than any of these large language models that also is not getting talked about enough. So we talked about an earlier question, AI shallow work automation has been critical. The other major development that we're so used to now, we forget, but I think is actually gonna be the key to this AI shallow work automation, but also all sorts of other ways of AI interests are life, is not these large language models, but it's what Google has in mind with Google Home. It's what Amazon has in mind with Alexa. These at-home appliances that you can talk to and ask to do things, they're ubiquitous. And they're ubiquitous in part because these companies made them very cheap. They wanted people to use them. Not trying to make a lot of money off them. Why? Why is Google or Amazon trying to get as many people as possible to use these agents at home that you could just talk to and it tries to understand you? It's data. The game they are playing is we want millions and millions of different people with different accents and different types of voices asking about all sorts of different things that we then try to understand. And we can then use this data set to train increasingly better interfaces that can understand natural language. That's the whole ball game. A chat GPT is pretty good at this. They figured out a new model. I don't want to get into the weeds. They have a human semi-supervised, semi-human supervised reinforcement learning model that they inserted during the GPT-T training to try to align its responses better with what's being asked. But this is the whole ball game is just natural language understanding. And Google is working on this and Amazon is working on this and Apple is working on this with their Siri devices. And this is what matters. Understanding people. Background activities, I think this is what we often get wrong. The actual activities that the disruptive AI in the future is going to do on our behalf are not that interesting. It's not, we're going to go right in RIA. It's, we're going to pull some data from an Excel table and email it to someone. It's, we're going to turn off the lights in your house because you, you know, said you were gone. It's really kind of boring stuff. All of the interesting stuff is understanding what you're saying. And that's why Google and Amazon and Apple invested so much money in the getting these devices everywhere is they want to train and generate the biggest possible data set of actually understanding what real people are saying. And figuring out, do we get it right or do we get it wrong? And if we look at examples and, and, and let's hand annotate these examples and figure out how our models work. And I think this is really going to be the slow creep of AI disruption. It's not going to be this one entity that suddenly takes over everyone's job. It's going to be that more and more devices in our world are increasingly better at understanding natural language questions, whether it be typed or spoken. And can then act accordingly, even if what we're asking to do is simple. We don't really need these agents to do really complicated things. We just need them to understand what we need. Most of what we do, that's a drag on our time. There's a drag on our energy is pretty simple. And it's something a machine could do if they just knew what it was we wanted. And so I think that's where we should be focusing is interfacing what matters. These 175 billion parameter models that can generate all this text is really not that interesting. I don't need a sign told script about bubble sort. I need you to understand me when I say, give me the email address of all of the students from my first section of my class. I need you just to understand what that means and be able to interface with the student database and get those emails out and format it properly so I can send the message to the class. That's what I want you to do. I don't want you to write an original poem in the style of Mary Oliver about a hockey game for my students. I need you to just go through when I say, look at the assignment pages for the problem sets. I assigned this semester, pull out the grading statistics and put them all into one document. Just to kind of, okay, I know what that means. And now I'm doing a pretty wrote automatic computer type thing. And I don't care if you come back and say, okay, I have three follow up questions so I understand what you mean. Do you mean this such and that such and this such. That's fine. Now let's take us another 10 or 15 seconds. I don't need in other words, how from 2001, I just need my basic computer to understand what I'm saying so I don't have to type it in or click on a lot of different buttons. So I mean, I think that's really where we're gonna see the big innovations is the slow creep of better and better human understanding plugged into relatively non-interesting actions. That's really where the stuff's gonna take a bigger and bigger picture. The disruption's gonna be more subtle. This idea that it's now this all at one large language model represents, we have this new self sufficient being that all at once will do everything. It's sexy, but I don't think that's the way this is gonna happen. All right, so let's change gears here, Jesse. I wanna do something interesting to wrap up the show. First, I wanna mention one of the long time sponsors that makes deep questions.

Chapter 8: Cal talks about Blinkist and Ladder Life
It's possible that's our friend's a blinkist. When you subscribe to Blinkist, you get access to 15-minute summaries of over 5,500 non-fiction books and podcasts, these short summaries which are called Blinks. You can either read them or you can listen to them while you do other activities. Jesse and I both use Blinkist as one of our primary tools for triaging which books we read. If there is a book we are interested in, we will read or listen to the Blink to see if it's worth buying. The other week, Jesse and I went through his own Blinkist list and we calculated that basically it was roughly what? 30% I think you said. 30% of the books for which you read Blinks have you go on the buy. But there we go. That is a case in point of the value of Blinkist. It really helps you hone in on the right books to buy. The books you don't buy, you still know there are main ideas. You can deploy them. So Blinkist is really a tool that any serious reader needs to add to their tool. So right now Blinkist has a special offer just for our audience. Go to Blinkist.com slash deep to start your 7 day free trial and you will get 45% off a Blinkist Premium membership. That's Blinkist spelled BLINK IST. Blinkist.com slash deep to get 45% off at a 7 day free trial. That's Blinkist.com slash deep. This offers good to April 30th only. In better news, for a limited time, you can take advantage of their Blinkist Connect program to share your premium account. You will in other words get two premium accounts for the price of one. So when you subscribe to Blinkist, you can give an account to a friend who you think would also appreciate it. So Blinkist.com slash deep to find out more. I also want to talk about latter. It's tax season. Tax season is when I often get stressed about putting things off because I don't even know where to get started. This got me thinking about the other classic thing that people put off which is getting life insurance. This is one of those things that you know you need but you don't know where to get started. Who do you talk to for life insurance? How much should life insurance talk? Costs. You're going to have to go to a doctor and get blood drawn and your eyeball scan before you can actually get a policy. And we get paralyzed by all this complexity and say forget it. Well this is where latter enters the scene. Ladder makes it easy to get life insurance. It's 100% digital. No doctors, no needles and no paperwork when you're applying for $3 million in coverage or less. You just answer a few questions about your health on an application. You need just a few minutes in a phone or laptop to apply. Ladder smart algorithms work in real time so you'll find out instantly if you're approved. No hidden fees, cancel any time, get a full refund if you change your mind in the first 30 days. Life insurance is only going to cost more as you get older so now is always the right time to get it. So go to ladderlife.com slash deep today to see if you're instantly approved. That's L A D D E R life dot com slash deep ladder life dot com slash deep. Or just let's do something interesting for those who are new to the show. This is the

Chapter 9: NPR leaves Twitter
segment where I take something that you sent me to my interesting at CalNewPort.com email address. You thought I might find interesting. I take something to caught my attention and I share it. So here's something a lot of people sent me. This is actually from just a couple days ago. I have it up on the screen now. So if you're listening, you can watch this at youtube.com plus CalNaport Media episode 244. It is an article from NPR with the following music to my ears headline. NPR quits Twitter. There's a whole backstory to this. NPR is having basically a feud with Twitter because Twitter started labeling the network as state affiliated media. The same description they use for propaganda outlets in Russia, China and other autocratic countries. That did not sit well with NPR. So then they changed it and said, okay, we'll call you government-funded media. But NPR said only 1% of our annual budget comes from federal funding. That's not really accurate either. You know what? Enough. They're walking away. And they put NPR politics, for example, put out a tweet that said all of our 52 Twitter accounts. We're not going to use them anymore. You want news for NPR? Subscribe to our email newsletter. Come to our website. Listen to our radio program. We'll keep you up to date. You don't have to use this other guys program. I really like to see that. Not because of the inter-seeing political battles between Musk and the political, the different media outlets. I mean, I wish they would just make this move even without that. But whatever gets them there, I think is good. As I've talked about so many times on this show before, it is baked into the architecture of Twitter that it is going to generate outrage. It is going to manipulate your emotions. It is going to create tribalism and is going to really color your understanding of the world, your understanding of current events in ways that are highly inaccurate and often highly inflammatory. It's just built into the collaborative curation mechanism of all these retweets combined with the power law network. We've talked about this before. It's not a great way to consumer-serimformation. Now, more and more outlets are doing this. A couple of weeks ago, we gave the example of the Washington Post Nationals baseball team coverage shifting away from live tweeting the games and instead having live updates on their Washington Post.com website. At the end of all those updates, they write a recap article and it's all packaged together and you can see how it unfolded and they have more different people writing and it unfolds in real time. I think the whole thing is great. There's no reason to be on someone else's platform mixing tweets about the baseball game with tweets about the Martian that's going to come and destroy the earth because you didn't give it hydroxychloroquine or whatever the hell else is going on on Twitter. Twitter was a lot of fun. It's a lot of engaging. It's a lot of very engaging. It's not the right way to consume news. It's not the right way to spread news. I'm really happy about this trend. I think we would be in a much calmer place as individuals. I think we'd be in a much calmer place politically. I think we'd be in a much calmer place just from a civic perspective. If more and more sources of news, if more and more sources of expression, if more and more sources of commentary, move to their own private gardens. Here's my website. Here's my podcast. Here's my newsletter. Not this giant mixing bowl where everyone and everything comes together in a homogenized interface. We have this distributed curation mechanism rapidly amplifying some things versus others. That's not a healthy way for a society to learn about things and communicate. I wrote about this in Wired Magazine early in the pandemic. I wrote an op-ed for Wired. I said, one of the number one things we could do for public health right now is we could get into the pandemic. We'd be shut down Twitter. I gave this argument in that article that, look, if professionals and if we retreated to websites, we could have considered long form articles that with rich links and the other types of articles and sources where the websites themselves, you would have the you could indicate authority by the fact that this website is hosted at a hospital or a known academic institution or a known news organization. We'd be able to curate on an individual sense of this website is at the reference in old SNL skit, clown penis.fart. It has weird gifts of it of eagles that are flying away with a song and a lot of it. I'm not going to pay as much attention to that to this long form article that's coming out of the Cleveland Clinic. I said, if we went back, humans will be much better at consuming information. The information will be much better presented. If we went back to more bespoke distributed communication as opposed to having everyone, the cranks and the experts and the weirdos and everybody all mixed together in the exact same interface with the exact same, their tweets look exactly the same and a completely dispassionate distribution mechanism rapidly amplifying things that are catching people's attention. We need to get away from that automation. We need to get away from that distribution and we get back to more bespoke things. We can learn from where you hosted. What does this look like? What are the things that have you written? We can really have context for information. Anyways, I don't want to go too far into this lecture, but basically this is a good trend. I think individually owned digital distribution of information, podcast, websites, blogs, newsletters is going to be a much healthier trend than saying, why don't we all just have one or two services that everyone uses? Good job, MPR. I hope more and more news sources follow your lead in the Washington Post's National's Reporters' Leagues. I think this is the right direction. Do you know the clown penis dot fart reference? No. It was in the late 90s. It was a classic SNL skit and it was an advertisement. They really had it. You know how these have those advertisements for brokerage firms? Yeah. Where it's super, a thought. I'm welcome. It's like Wilford Brimley. Welcome to such and such financial partners where trust is our number one, whatever. It's like this real serious ad. They're like, you can find out more at our website, clown penis dot fart. Then they kind of reveal by the time we set up our website, it was the only address left. It was like the premise of it is this really serious financial brokerage firm. In the late 90s, it felt like all the URLs were gone. It was the only URL that was left. It was just a very serious commercial that kept having to say clown penis dot fart. Classic SNL. All right. I'm tired. Too much AI. I'm happy now to talk about AI whenever you want. My gag order has been lifted, but maybe it'll be a while until we get too much more deeply into that. But thank you, Wulf, and Lissie. Thank you all for putting up with that. We'll be back next week with the next, hopefully much less computer science filled episode of the podcast and until then as always stay deep.